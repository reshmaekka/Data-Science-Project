{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d85d37-4af3-4cff-ba5d-483955f19c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\USER/nltk_data', 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data', 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data', 'C:\\\\Users\\\\USER\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\USER\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "##importing library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from urllib.request import urlopen as uReq\n",
    "import urllib\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# Set the NLTK data path\n",
    "nltk.data.path.append('C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', download_dir='C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "nltk.download('stopwords', download_dir='C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "nltk.download('wordnet', download_dir='C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Check if NLTK data path is correctly set\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f71e416-702b-4b86-a874-4d52828fdd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of              URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('input1.csv')\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caf5983-8dd9-4534-90a2-6196e58ac3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('input1.csv')[['URL_ID','URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfbf4bb-4d9f-479e-be63-7777c2e85846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b8fa43-4048-4ff2-9458-b19d09eff305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('URL_ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0244ad-6cf2-49a6-9901-f1f74d5c206c",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd201d54-f9a5-4bb1-89ed-fb9ddc270386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#data extraction\\nimport pandas as pd\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\ndef extract_article(url):\\n    response = requests.get(url)\\n    soup = BeautifulSoup(response.text, \\'html.parser\\')\\n    title = soup.find(\\'title\\').text\\n    content = soup.find(\"div\", class_=\"td-post-content tagdiv-type\")\\n    \\n    if content:\\n        text = content.get_text(separator=\\' \\', strip=True)\\n        return title, text\\n    return title, \"\"\\n\\ndef save_article(url_id, title, text):\\n    filename = f\\'{url_id}.txt\\'\\n    with open(filename, \\'w\\', encoding=\\'utf-8\\') as file:\\n        file.write(title + \"\\n\" + text)\\n\\ndef main():\\n    df = pd.read_excel(\\'Input.xlsx\\')\\n    for index, row in df.iterrows():\\n        url_id = row[\\'URL_ID\\']\\n        url = row[\\'URL\\']\\n        title, text = extract_article(url)\\n        save_article(url_id, title, text)\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#data extraction\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title = soup.find('title').text\n",
    "    content = soup.find(\"div\", class_=\"td-post-content tagdiv-type\")\n",
    "    \n",
    "    if content:\n",
    "        text = content.get_text(separator=' ', strip=True)\n",
    "        return title, text\n",
    "    return title, \"\"\n",
    "\n",
    "def save_article(url_id, title, text):\n",
    "    filename = f'{url_id}.txt'\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(title + \"\\n\" + text)\n",
    "\n",
    "def main():\n",
    "    df = pd.read_excel('Input.xlsx')\n",
    "    for index, row in df.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        url = row['URL']\n",
    "        title, text = extract_article(url)\n",
    "        save_article(url_id, title, text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891049d8-5d02-4c3e-bb54-b1bfd69f5559",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648f8e09-1016-414e-a139-c3847d08e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing extract files\n",
    "text = pd.read_csv('C:/Users/USER/OneDrive/Desktop/BlackCoffer/blackassign0100.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf00b1e8-44c8-452f-810d-6b641b1f9424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 136.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#information of data Frame\n",
    "text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491c4610-b893-4397-81ac-d6794ef7e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing extra created column\n",
    "#text.drop(1,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3647dcf-5e12-4a9b-b0f5-d431b79ff90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting type\n",
    "text=text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c72180d-4dc8-44f0-8e33-13a8830ba334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text to sentence\n",
    "import re\n",
    "a=text[0].str.split('([\\.]\\s)',expand=False)#splitting text on '.'\n",
    "b=a.explode()#converting to rows\n",
    "b=pd.DataFrame(b)#creating data frame\n",
    "b.columns=['abc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a316924e-db0f-4b60-8ed0-9b05d99b945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing . char from each rows\n",
    "def abcd(x):    \n",
    "    nopunc =[char for char in x if char != '.']\n",
    "    return ''.join(nopunc)\n",
    "b['abc']=b['abc'].apply(abcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e53e38-6194-418f-88a0-1e2d40fdaebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing emty space with null values\n",
    "c=b.replace('',np.nan,regex=True)\n",
    "c=c.mask(c==\" \")\n",
    "c=c.dropna()\n",
    "c.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd5744b6-3675-4052-9eb2-49e076a22061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How will COVID-19 affect the world of work? - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 abc\n",
       "0  How will COVID-19 affect the world of work? - ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592b9a9a-b4ae-4e42-a356-c46ee9903fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nltk library and stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9edb20da-839b-465d-9968-36ee6bac690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=[punc for punc in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02218f4b-3f74-4ce6-81ab-8b503131fb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0192af1-ae87-45fd-b34b-2d6a83551c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c12b95d-22b6-4b3f-a035-a96c308e403f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install openpyxl\\n!pip install requests'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install openpyxl\n",
    "!pip install requests'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4196f4f2-4f27-4b2b-923e-ced156cfc144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a37b43-4ce6-4cbf-92f9-c26391bd8b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/1.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/2.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/3.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/4.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/5.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/6.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/7.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/8.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/9.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/10.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/11.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/12.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/13.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/14.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/15.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/16.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/17.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/18.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/19.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/20.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/21.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/22.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/23.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/24.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/25.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/26.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/27.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/28.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/29.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/30.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/31.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/32.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/33.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/34.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/35.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/36.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/37.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/38.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/39.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/40.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/41.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/42.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/43.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/44.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/45.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/46.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/47.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/48.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/49.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/50.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/51.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/52.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/53.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/54.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/55.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/56.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/57.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/58.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/59.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/60.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/61.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/62.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/63.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/64.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/65.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/66.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/67.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/68.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/69.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/70.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/71.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/72.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/73.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/74.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/75.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/76.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/77.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/78.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/79.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/80.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/81.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/82.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/83.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/84.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/85.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/86.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/87.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/88.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/89.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/90.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/91.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/92.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/93.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/94.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/95.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/96.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/97.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/98.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/99.txt not found. Skipping.\n",
      "File C:/Users/USER/OneDrive/Desktop/BlackCoffer/100.txt not found. Skipping.\n"
     ]
    }
   ],
   "source": [
    " import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the text data\n",
    "text_data = []\n",
    "\n",
    "# Assuming you have saved files named 1.txt, 2.txt, ..., you can read them\n",
    "num_files = 100  # Adjust based on the number of files you have\n",
    "for i in range(1, num_files + 1):\n",
    "    filename = f\"C:/Users/USER/OneDrive/Desktop/BlackCoffer/{i}.txt\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            text_data.append({'id': i, 'text': text})\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45cbd406-d359-4d27-89d6-96cb3f97457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize NLTK stopwords\n",
    "nltk.data.path.append('C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "#nltk.download('punkt', download_dir='C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "#nltk.download('stopwords', download_dir='C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to compute variables for each text\n",
    "def compute_variables(text):\n",
    "    # Tokenization and word count\n",
    "    tokens = word_tokenize(text)\n",
    "    word_count = len(tokens)\n",
    "    \n",
    "    # Stopword count\n",
    "    stopwords_count = sum(1 for word in tokens if word.lower() in stop_words)\n",
    "    \n",
    "    # Average word length\n",
    "    if word_count > 0:\n",
    "        avg_word_length = sum(len(word) for word in tokens) / word_count\n",
    "    else:\n",
    "        avg_word_length = 0\n",
    "    \n",
    "    # Sentiment analysis using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_polarity = blob.sentiment.polarity\n",
    "    sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    return {\n",
    "        'Word Count': word_count,\n",
    "        'Stopword Count': stopwords_count,\n",
    "        'Average Word Length': avg_word_length,\n",
    "        'Sentiment Polarity': sentiment_polarity,\n",
    "        'Sentiment Subjectivity': sentiment_subjectivity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52b683ff-a200-4550-9d13-9ef0031ab098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport requests\\nfrom bs4 import BeautifulSoup as bs\\nimport urllib.parse\\n\\nBlank_link = {}\\n\\nclass data_ingestion:\\n   \\n   def secondary(self):\\n       data = pd.read_excel(\\'Input.xlsx\\')\\n       df = data.copy()\\n       # Create an empty article_words column\\n       df[\\'article_words\\'] = \\'\\'\\n       \\n       for i, url in enumerate(df[\\'URL\\']):\\n           response_code = requests.get(url)\\n           soup = bs(response_code.text, \\'html.parser\\')\\n           article_title = soup.find(\\'title\\').text\\n           all_text_element = soup.find(\"div\", class_=\"td-post-content tagdiv-type\")\\n           \\n           if all_text_element is not None:\\n               all_text = all_text_element.get_text(strip=True, separator=\\'\\n\\')\\n               firstdata = all_text.splitlines()\\n           else:\\n               print(f\"No matching element found in the html for url: {url}\")\\n               Blank_link[f\"blackassign00{i+1}\"] = url\\n               firstdata = []\\n               for key, value in Blank_link.items():\\n                   if url == value:\\n                       response_code = requests.get(value)\\n                       soup = bs(response_code.text, \\'html.parser\\')\\n                       all_text_element = soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\\n                       \\n                       if all_text_element is not None:\\n                           all_text = all_text_element.get_text(strip=True, separator=\\'\\n\\')\\n                           firstdata = all_text.splitlines()\\n                           break\\n                       else:\\n                           firstdata = []\\n                           break\\n\\n               if not firstdata:\\n                   print(f\"No matching element found in the html for url: {url}\")\\n                   filename = urllib.parse.quote_plus(url)\\n                   file_path = \\'C:/Users/USER/OneDrive/Desktop/BlackCoffer\\'\\n                   space = \"\\n\"\\n                   with open(f\"{file_path}/{filename}.txt\", \\'w+\\') as file1:\\n                       file1.writelines(article_title + space)\\n                       file1.writelines(space.join(firstdata))\\n           \\n           # Update article_words column for the current row\\n           df.at[i, \\'article_words\\'] = f\"{article_title} - {firstdata}\"\\n       \\n       df.to_csv(\\'input_csv.csv\\', index=False)\\n       return df\\n\\nif __name__ == \"__main__\":\\n   obj = data_ingestion()\\n   obj.secondary()'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " '''import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.parse\n",
    "\n",
    "Blank_link = {}\n",
    "\n",
    "class data_ingestion:\n",
    "    \n",
    "    def secondary(self):\n",
    "        data = pd.read_excel('Input.xlsx')\n",
    "        df = data.copy()\n",
    "        # Create an empty article_words column\n",
    "        df['article_words'] = ''\n",
    "        \n",
    "        for i, url in enumerate(df['URL']):\n",
    "            response_code = requests.get(url)\n",
    "            soup = bs(response_code.text, 'html.parser')\n",
    "            article_title = soup.find('title').text\n",
    "            all_text_element = soup.find(\"div\", class_=\"td-post-content tagdiv-type\")\n",
    "            \n",
    "            if all_text_element is not None:\n",
    "                all_text = all_text_element.get_text(strip=True, separator='\\n')\n",
    "                firstdata = all_text.splitlines()\n",
    "            else:\n",
    "                print(f\"No matching element found in the html for url: {url}\")\n",
    "                Blank_link[f\"blackassign00{i+1}\"] = url\n",
    "                firstdata = []\n",
    "                for key, value in Blank_link.items():\n",
    "                    if url == value:\n",
    "                        response_code = requests.get(value)\n",
    "                        soup = bs(response_code.text, 'html.parser')\n",
    "                        all_text_element = soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
    "                        \n",
    "                        if all_text_element is not None:\n",
    "                            all_text = all_text_element.get_text(strip=True, separator='\\n')\n",
    "                            firstdata = all_text.splitlines()\n",
    "                            break\n",
    "                        else:\n",
    "                            firstdata = []\n",
    "                            break\n",
    "\n",
    "                if not firstdata:\n",
    "                    print(f\"No matching element found in the html for url: {url}\")\n",
    "                    filename = urllib.parse.quote_plus(url)\n",
    "                    file_path = 'C:/Users/USER/OneDrive/Desktop/BlackCoffer'\n",
    "                    space = \"\\n\"\n",
    "                    with open(f\"{file_path}/{filename}.txt\", 'w+') as file1:\n",
    "                        file1.writelines(article_title + space)\n",
    "                        file1.writelines(space.join(firstdata))\n",
    "            \n",
    "            # Update article_words column for the current row\n",
    "            df.at[i, 'article_words'] = f\"{article_title} - {firstdata}\"\n",
    "        \n",
    "        df.to_csv('input_csv.csv', index=False)\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    obj = data_ingestion()\n",
    "    obj.secondary()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a526545d-f4df-4e03-a434-e8581de38cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 was the year the world was ravaged by the SarsCov2 virus. This notorious virus brought about a pandemic that would go on to change the course of humanity.  From that point forth daily lives of everyone across the world changed. With widespread stringent lockdowns, the entire world came to a sharp halt. Not only was the general populace affected, but the pandemic also affected all industries. The pandemic did not even spare critical industries, like healthcare and security. While these industries were required to function for the benefit of society, their daily operations changed drastically. But just as human nature prevails, we rose from this adversity. Post pandemic era saw the rise of new technologies that could aid overcome the restrictions put forth by the pandemic. In this article, we will specifically focus on the healthcare industries, innovations done in the industry, and the impact of those innovations on humans by 2030.\n",
      "Lockdowns initiated to curb the pandemic caused people to stay in their homes at all times. The exception to this was the medical staff who acted as our first line of defense against the pandemic. They not only had to deal with the pandemic but also had to cater to patients suffering from various other ailments. While people were restricted to go out, they still required medical consultation for their issues. This conundrum led to the rise of imparting healthcare facilities over electronic media. Healthcare workers utilized technologies such as video calls and virtual meetings to reach out to the needy. Now, these practices are also known as ‘e-Health’.  As the pandemic raged on, society saw numerous initiatives in the e-health sector. Government-led initiatives like ‘eSanjeevani OPD’ brought government OPD facilities directly to people’s homes.  It should be noted that e-health is not just limited to providing patients with virtual meetings. It encompasses every technology that transitions the traditional healthcare sector into a more accessible electronic form. Keeping with this trend we also saw the use of the Internet of Things (IoT)  to build the Internet of Medical Things (IoMT). These internet-connected devices allowed remote monitoring and controlling of facilities imparted to patients.  With the onset of 5G technology, the use of  IoMT has flourished. Some of these solutions include smart devices that monitor an individual’s health metrics. These devices are also capable of sending out alerts in case the metrics do not fall under the acceptable range. Furthermore with the advancements in artificial intelligence, machine learning, and deep learning technologies these devices do more than just report. With the help of these technologies, IoMT devices are now capable of intelligently performing actions that keep the metric of a patient in check.\n",
      "Another interesting technology that has found its use in the healthcare industry is ‘Blockchain technology. The technology used as the base for cryptocurrency can also be used to maintain immutable records of patients. Such blockchain solutions are already finding their way into the market Pranacare, is one such India start-up. It is a  platform for doctors driven by blockchain and AI. It offers tools to help dieticians, diabetologists, and cardiac specialists manage their customers and data. It also maintains, tracks, and records patient data and offers a decentralized ledger. The armchair is another organization that combines blockchain technology and the health industry.  The armchair provides an Ethereum-based platform for electronic health records storage. The blockchain is a hybrid public/private network that analyses data using Artificial Intelligence. Will use Ethereum Smart Contracts with the Hyperledger Sawtooth platform to offer a safe method for both patients and providers to access patient data. Existing e-health services also saw a rise during the pandemic. For example, telemedicine visits increased from 1-2% of ambulatory care visits before to the pandemic to 30% of all visits. With customer readiness to adopt telehealth climbing to 66%. According to health systems, up to 40% of primary care appointments might be performed remotely.\n",
      "With the increased penetration of the internet in our daily lives, it has been now critical for companies and start-ups to capture this segment. Keeping in with this agenda, the market has seen a proliferation of companies providing e-health services. Consequently, this industry has also seen an increase.in investments. Between 2019 and 2020, investments in telemedicine solutions quadrupled, rising from $1.1 billion to $3.1 billion. In 2020, total funding for remote patient monitoring (RPM) solutions will have more than quadrupled, rising from $417 million to $941 million. According to e-healthcare investors, RPM solutions for chronic care management are expected to become more popular in the next coming years. Analysts predict that eHealth will increase at a compound annual growth rate of 16.1% from 2022 to 2030, with the eHealth segment expected to generate $61.4 billion in sales in 2023 alone. China is expected to lead the global market followed by the USA. Overall, the e-Health industry is expected to see a boom in investments, providing early investors with a large return for their investments.\n",
      "While e-Health seems like the probable future for the entire healthcare industry, special attention must be given to the security aspect of these services. Being connected to the internet and being made available to all also attracts a lot of nefarious elements. Cybercriminals continually target the healthcare industry. The ransomware attack on AIIMS New Delhi is a prime example of this. Cybercriminals were able to cripple AIIMS systems. For over two weeks this caused massive chaos and confusion among the general public. The pandemic itself saw a large number of healthcare-themed smishing and vishing attacks. Their major motive was to exploit the insecure and scared public and make use of the pandemic to extort money. Every example of a cyberattack on a healthcare industry has always led to massive chaos. Healthcare is considered a critical industry, thereby it needs to be adequately protected. With e-Health on the rise, by 2023 society can also expect a lot of jobs to open in the sector of cyber security in the healthcare industry.\n",
      "e-Health solutions are the need of the hour. Integrating e-Health solutions with up-and-coming technologies can effectively ensure inclusivity for all. Such solutions can bring healthcare facilities to the remotest parts of the earth. With current trends in sight, 2030 is expected to have a large market share occupied by e-Health solutions. Early investors can expect high returns by 2030. With this in mind, special focus should also be given to securing the ever-growing e-health industry.\n",
      "Blackcoffer Insights 46: Aparajita Thakur, Jesus And Mary College , University Of Delhi\n"
     ]
    }
   ],
   "source": [
    "url='https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/'\n",
    "response_code = requests.get(url)\n",
    "soup =bs(response_code.text,'html.parser')\n",
    "article_title=soup.find('title').text\n",
    "#find the element with the specified class\n",
    "alldiv_element=soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
    "#check if the element exists\n",
    "if alldiv_element:\n",
    "    alldiv=alldiv_element.get_text(strip=True, separator='\\n')\n",
    "else:\n",
    "    print(f\"No matching element found for url:{url}\")\n",
    "    alldiv=\"\"\n",
    "    #print or use the 'alldiv' variable as needed\n",
    "print(alldiv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e52bf77d-2d67-4b98-93d4-bf595f40644b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['URL_ID', 'URL'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c32e78d-b3fc-4f5c-a805-c759ef9cb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blank_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "217489c0-9099-4969-9e3b-6e808c8a168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/Users/USER/OneDrive/Desktop/BlackCoffer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4936aed2-49b7-42fc-a2ae-19d998881b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'updated_list=[]\\n\\nfor i, j in Blank_link.items():\\n    response_code=requests.get(j)\\n    soup=bs(response_code.text, \\'html.parser\\')\\n    article_title=soup.find(\\'title\\').text\\n\\n    alldiv=soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\\n\\n    if alldiv is not None:\\n        firstdata=alldiv.text\\n        filename=urllib.parse.quote_plus(j)\\n        filepath=\\'C:/Users/RANU RAJA/Desktop/Blockcoffer internship project\\'\\n        space=\" \"\\n\\n        with open(f\"{file_path}\\\\{filename}.txt\", \\'w+\\') as file1:\\n                file1.writelines(article_title)\\n                file1.writelines(space)\\n                file1.writelines(firstdata)\\n        updated_dict={\\n            \\'URL_ID\\':i,\\n            \\'URL\\':j,\\n            \\'article_words\\':f\"{article_title} - {firstdata}\"\\n        }\\n        updated_list.append(updated_dict)\\n    else:\\n        print(f\"No data available for the link: {j}\")\\nupdated_df=pd.DataFrame(updated_list)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''updated_list=[]\n",
    "\n",
    "for i, j in Blank_link.items():\n",
    "    response_code=requests.get(j)\n",
    "    soup=bs(response_code.text, 'html.parser')\n",
    "    article_title=soup.find('title').text\n",
    "\n",
    "    alldiv=soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
    "\n",
    "    if alldiv is not None:\n",
    "        firstdata=alldiv.text\n",
    "        filename=urllib.parse.quote_plus(j)\n",
    "        filepath='C:/Users/RANU RAJA/Desktop/Blockcoffer internship project'\n",
    "        space=\" \"\n",
    "\n",
    "        with open(f\"{file_path}\\{filename}.txt\", 'w+') as file1:\n",
    "                file1.writelines(article_title)\n",
    "                file1.writelines(space)\n",
    "                file1.writelines(firstdata)\n",
    "        updated_dict={\n",
    "            'URL_ID':i,\n",
    "            'URL':j,\n",
    "            'article_words':f\"{article_title} - {firstdata}\"\n",
    "        }\n",
    "        updated_list.append(updated_dict)\n",
    "    else:\n",
    "        print(f\"No data available for the link: {j}\")\n",
    "updated_df=pd.DataFrame(updated_list)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "394fd665-688e-406f-8b7c-430d580bf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain_data=pd.DataFrame(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cccdffa-512a-466d-94ee-1672efaa9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21cbf770-bc7a-4435-964d-198e41ef8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.merge(df, updated_df[['URL','article_words']], on='URL', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a6ce90d-4ae2-4bbd-921f-1d8ac9665796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain_data.to_csv('Input1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abe9cbbb-7865-4b67-ab41-2bbcb67e5ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"remain_data.set_index('URL_ID', inplace=True)\\ndf.set_index('URL_ID',inplace=True)\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''remain_data.set_index('URL_ID', inplace=True)\n",
    "df.set_index('URL_ID',inplace=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a17f289-2eff-4b50-ad9c-a7cb859a695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9abbc949-a50f-422f-a9a9-8a62d4fce600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remain_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cccebd83-64ae-431e-bfdc-5cccaa62e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data=df.combine_first(remain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a27d558-256a-4251-8295-7637cb625f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6130dc24-1d80-49da-955a-aabeaec932d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.to_csv(\"Input1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f24db2b6-f928-400d-a09a-f053dd2575e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again try to perform analysis on data and performing operations\n",
    "text=pd.read_csv('C:/Users/USER/OneDrive/Desktop/BlackCoffer/blackassign0100.txt',header=None)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed1d7c10-37d7-45b2-a3cd-be9dfea4cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 136.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d38ff56-e614-474f-8c83-54e7b4bae7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('input1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fa8a8bb-eca6-410e-ba77-2d973f2ce89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stop words files that are provided\n",
    "stopwords_Names=open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/StopWords/StopWords_Names.txt','r',encoding='ISO-8859-1')\n",
    "stopwords_Auditor=open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/StopWords/StopWords_Auditor.txt','r',encoding='ISO-8859-1')\n",
    "stopwords_Currencies=open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/StopWords/StopWords_Currencies.txt','r',encoding='ISO-8859-1')\n",
    "stopwords_DatesandNumbers=open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/StopWords/StopWords_DatesandNumbers.txt','r',encoding='ISO-8859-1')\n",
    "stopwords_Generic=open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/StopWords/StopWords_Generic.txt','r',encoding='ISO-8859-1')\n",
    "stopwords_GenericLong=open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/StopWords/StopWords_GenericLong.txt','r',encoding='ISO-8859-1')\n",
    "stopwords_Geographic=open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/StopWords/StopWords_Geographic.txt','r',encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d26cc5a-9d69-4a9b-844f-08557850b463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST\\n',\n",
       " 'YOUNG\\n',\n",
       " 'DELOITTE\\n',\n",
       " 'TOUCHE\\n',\n",
       " 'KPMG\\n',\n",
       " 'PRICEWATERHOUSECOOPERS\\n',\n",
       " 'PRICEWATERHOUSE\\n',\n",
       " 'COOPERS\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_Auditor.seek(0)\n",
    "stopwords_Auditor.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67d848ca-eb68-4376-8ab0-fe58d667edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating func for removing stop words\n",
    "def text_process(text):\n",
    "    nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','’','?']]\n",
    "    nopunc=''.join(nopunc)\n",
    "    txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])\n",
    "    txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])\n",
    "    txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])\n",
    "    txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])\n",
    "    txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])\n",
    "    txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])\n",
    "    return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865c49f-b490-4179-83f4-948cd319757b",
   "metadata": {},
   "source": [
    "## Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "781adad0-e106-40ec-b165-7cb9177a6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob\n",
    "# Load stopwords and sentiment word lists\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "positive= set(open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/MasterDictionary/positive-words.txt', 'r').read().splitlines())\n",
    "negative= set(open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/MasterDictionary/negative-words.txt', 'r').read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6dec072-f00d-4a63-b84c-df7bcdbdf38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "positive = pd.DataFrame(positive, columns=['abc'])\n",
    "negative = pd.DataFrame(negative, columns=['abc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b585503-7fee-4201-8ff1-191914d88cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns to 'abc'\n",
    "positive.columns = ['abc']\n",
    "negative.columns = ['abc']\n",
    "\n",
    "# Convert the 'abc' column to string type\n",
    "positive['abc'] = positive['abc'].astype(str)\n",
    "negative['abc'] = negative['abc'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b504ac65-579e-49f6-98ef-9ea075e138c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive list\n",
    "length=positive.shape[0]\n",
    "post=[]\n",
    "for i in range(0,length):\n",
    "   nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']\n",
    "   nopunc=''.join(nopunc)\n",
    "\n",
    "   post.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b4efa35-5b23-496f-9182-d3386fe85064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative list\n",
    "length=negative.shape[0]\n",
    "neg=[]\n",
    "for i in range(0,length):\n",
    "  nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']\n",
    "  nopunc=''.join(nopunc)\n",
    "  neg.append(nopunc)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d76ea91-63fd-4068-a02a-ab70bdd0a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tokenize library\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed9167e7-3edb-41bb-9875-ad1c208994e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list=[]\n",
    "length=c.shape[0]\n",
    "for i in range(0,length):\n",
    "  txt=' '.join([word for word in c.iloc[i]])\n",
    "  txt_list.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d270b349-5509-41e0-88c3-b4ab360ed0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization of text\n",
    "tokenize_text=[]\n",
    "for i in txt_list:\n",
    "  \n",
    "  tokenize_text+=(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c92bcdd3-f1f7-46bc-928a-35f0923da8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'will', 'COVID-19', 'affect', 'the', 'world', 'of', 'work', '?', '-', 'Blackcoffer', 'Insights']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "537256ec-38d4-43a7-8803-09fa8144e3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b6c9376-47a3-474c-96b9-4cb45e561789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_corpus(x):\n",
    "    string_format = str(x).lower()\n",
    "    lower_words=re.sub('[^a-zA-Z]+',' ',string_format).strip()\n",
    "    #lower_words=lower_words.split()\n",
    "    token=word_tokenize(lower_words)\n",
    "    token_word=[t for t in tokens if t not in stop_words]\n",
    "    lemantizzed=[lem.lemmatize(w) for w in token_words]\n",
    "    return lemantizzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3de4e988-00e5-42de-a2c3-efcb1a349e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_corpus(x):\n",
    "    string_format = str(x).lower()\n",
    "    lower_words=re.sub('[^a-zA-Z]+',' ',string_format).strip()\n",
    "    #lower_words=lower_words.split()\n",
    "    token = word_tokenize(lower_words)\n",
    "    token_word = [t for t in token if t not in (stopwords_Auditor,stopwords_Currencies,stopwords_DatesandNumbers,stopwords_Generic,stopwords_GenericLong,stopwords_Geographic,stopwords_Names)]\n",
    "    lemantizzed=[lem.lemmatize(w) for w in token_words]\n",
    "    return lemantizzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a9be86d-9906-4b78-acdb-137c6dcf9828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_neg = open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/MasterDictionary/negative-words.txt','r',encoding='ISO-8859-1')\n",
    "file_neg.seek(0)\n",
    "neg_split = file_neg.read().split()\n",
    "len(neg_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8df09196-f9c1-42d3-b6c3-e2721dab20e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_pos = open('C:/Users/USER/OneDrive/Desktop/BlackCoffer/MasterDictionary/positive-words.txt','r',encoding='ISO-8859-1')\n",
    "file_pos.seek(0)\n",
    "pos_split = file_pos.read().split()\n",
    "len(pos_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "08413036-41cb-4dab-9f78-523ea011a1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborts',\n",
       " 'abrade',\n",
       " 'abrasive',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abscond',\n",
       " 'absence',\n",
       " 'absent-minded',\n",
       " 'absentee',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'accidental',\n",
       " 'accost',\n",
       " 'accursed',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accusingly',\n",
       " 'acerbate',\n",
       " 'acerbic',\n",
       " 'acerbically',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'aches',\n",
       " 'achey',\n",
       " 'aching',\n",
       " 'acrid',\n",
       " 'acridly',\n",
       " 'acridness',\n",
       " 'acrimonious',\n",
       " 'acrimoniously',\n",
       " 'acrimony',\n",
       " 'adamant',\n",
       " 'adamantly',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addicts',\n",
       " 'admonish',\n",
       " 'admonisher',\n",
       " 'admonishingly',\n",
       " 'admonishment',\n",
       " 'admonition',\n",
       " 'adulterate',\n",
       " 'adulterated',\n",
       " 'adulteration',\n",
       " 'adulterier',\n",
       " 'adversarial',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'afflict',\n",
       " 'affliction',\n",
       " 'afflictive',\n",
       " 'affront',\n",
       " 'afraid',\n",
       " 'aggravate',\n",
       " 'aggravating',\n",
       " 'aggravation',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressiveness',\n",
       " 'aggressor',\n",
       " 'aggrieve',\n",
       " 'aggrieved',\n",
       " 'aggrivation',\n",
       " 'aghast',\n",
       " 'agonies',\n",
       " 'agonize',\n",
       " 'agonizing',\n",
       " 'agonizingly',\n",
       " 'agony',\n",
       " 'aground',\n",
       " 'ail',\n",
       " 'ailing',\n",
       " 'ailment',\n",
       " 'aimless',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alarmingly',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'alienation',\n",
       " 'allegation',\n",
       " 'allegations',\n",
       " 'allege',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'aloof',\n",
       " 'altercation',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambivalence',\n",
       " 'ambivalent',\n",
       " 'ambush',\n",
       " 'amiss',\n",
       " 'amputate',\n",
       " 'anarchism',\n",
       " 'anarchist',\n",
       " 'anarchistic',\n",
       " 'anarchy',\n",
       " 'anemic',\n",
       " 'anger',\n",
       " 'angrily',\n",
       " 'angriness',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'animosity',\n",
       " 'annihilate',\n",
       " 'annihilation',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyances',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annoys',\n",
       " 'anomalous',\n",
       " 'anomaly',\n",
       " 'antagonism',\n",
       " 'antagonist',\n",
       " 'antagonistic',\n",
       " 'antagonize',\n",
       " 'anti-',\n",
       " 'anti-american',\n",
       " 'anti-israeli',\n",
       " 'anti-occupation',\n",
       " 'anti-proliferation',\n",
       " 'anti-semites',\n",
       " 'anti-social',\n",
       " 'anti-us',\n",
       " 'anti-white',\n",
       " 'antipathy',\n",
       " 'antiquated',\n",
       " 'antithetical',\n",
       " 'anxieties',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'anxiousness',\n",
       " 'apathetic',\n",
       " 'apathetically',\n",
       " 'apathy',\n",
       " 'apocalypse',\n",
       " 'apocalyptic',\n",
       " 'apologist',\n",
       " 'apologists',\n",
       " 'appal',\n",
       " 'appall',\n",
       " 'appalled',\n",
       " 'appalling',\n",
       " 'appallingly',\n",
       " 'apprehension',\n",
       " 'apprehensions',\n",
       " 'apprehensive',\n",
       " 'apprehensively',\n",
       " 'arbitrary',\n",
       " 'arcane',\n",
       " 'archaic',\n",
       " 'arduous',\n",
       " 'arduously',\n",
       " 'argumentative',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrogantly',\n",
       " 'ashamed',\n",
       " 'asinine',\n",
       " 'asininely',\n",
       " 'asinininity',\n",
       " 'askance',\n",
       " 'asperse',\n",
       " 'aspersion',\n",
       " 'aspersions',\n",
       " 'assail',\n",
       " 'assassin',\n",
       " 'assassinate',\n",
       " 'assault',\n",
       " 'assult',\n",
       " 'astray',\n",
       " 'asunder',\n",
       " 'atrocious',\n",
       " 'atrocities',\n",
       " 'atrocity',\n",
       " 'atrophy',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'audacious',\n",
       " 'audaciously',\n",
       " 'audaciousness',\n",
       " 'audacity',\n",
       " 'audiciously',\n",
       " 'austere',\n",
       " 'authoritarian',\n",
       " 'autocrat',\n",
       " 'autocratic',\n",
       " 'avalanche',\n",
       " 'avarice',\n",
       " 'avaricious',\n",
       " 'avariciously',\n",
       " 'avenge',\n",
       " 'averse',\n",
       " 'aversion',\n",
       " 'aweful',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awfulness',\n",
       " 'awkward',\n",
       " 'awkwardness',\n",
       " 'ax',\n",
       " 'babble',\n",
       " 'back-logged',\n",
       " 'back-wood',\n",
       " 'back-woods',\n",
       " 'backache',\n",
       " 'backaches',\n",
       " 'backaching',\n",
       " 'backbite',\n",
       " 'backbiting',\n",
       " 'backward',\n",
       " 'backwardness',\n",
       " 'backwood',\n",
       " 'backwoods',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'baffle',\n",
       " 'baffled',\n",
       " 'bafflement',\n",
       " 'baffling',\n",
       " 'bait',\n",
       " 'balk',\n",
       " 'banal',\n",
       " 'banalize',\n",
       " 'bane',\n",
       " 'banish',\n",
       " 'banishment',\n",
       " 'bankrupt',\n",
       " 'barbarian',\n",
       " 'barbaric',\n",
       " 'barbarically',\n",
       " 'barbarity',\n",
       " 'barbarous',\n",
       " 'barbarously',\n",
       " 'barren',\n",
       " 'baseless',\n",
       " 'bash',\n",
       " 'bashed',\n",
       " 'bashful',\n",
       " 'bashing',\n",
       " 'bastard',\n",
       " 'bastards',\n",
       " 'battered',\n",
       " 'battering',\n",
       " 'batty',\n",
       " 'bearish',\n",
       " 'beastly',\n",
       " 'bedlam',\n",
       " 'bedlamite',\n",
       " 'befoul',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'beggarly',\n",
       " 'begging',\n",
       " 'beguile',\n",
       " 'belabor',\n",
       " 'belated',\n",
       " 'beleaguer',\n",
       " 'belie',\n",
       " 'belittle',\n",
       " 'belittled',\n",
       " 'belittling',\n",
       " 'bellicose',\n",
       " 'belligerence',\n",
       " 'belligerent',\n",
       " 'belligerently',\n",
       " 'bemoan',\n",
       " 'bemoaning',\n",
       " 'bemused',\n",
       " 'bent',\n",
       " 'berate',\n",
       " 'bereave',\n",
       " 'bereavement',\n",
       " 'bereft',\n",
       " 'berserk',\n",
       " 'beseech',\n",
       " 'beset',\n",
       " 'besiege',\n",
       " 'besmirch',\n",
       " 'bestial',\n",
       " 'betray',\n",
       " 'betrayal',\n",
       " 'betrayals',\n",
       " 'betrayer',\n",
       " 'betraying',\n",
       " 'betrays',\n",
       " 'bewail',\n",
       " 'beware',\n",
       " 'bewilder',\n",
       " 'bewildered',\n",
       " 'bewildering',\n",
       " 'bewilderingly',\n",
       " 'bewilderment',\n",
       " 'bewitch',\n",
       " 'bias',\n",
       " 'biased',\n",
       " 'biases',\n",
       " 'bicker',\n",
       " 'bickering',\n",
       " 'bid-rigging',\n",
       " 'bigotries',\n",
       " 'bigotry',\n",
       " 'bitch',\n",
       " 'bitchy',\n",
       " 'biting',\n",
       " 'bitingly',\n",
       " 'bitter',\n",
       " 'bitterly',\n",
       " 'bitterness',\n",
       " 'bizarre',\n",
       " 'blab',\n",
       " 'blabber',\n",
       " 'blackmail',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blameworthy',\n",
       " 'bland',\n",
       " 'blandish',\n",
       " 'blaspheme',\n",
       " 'blasphemous',\n",
       " 'blasphemy',\n",
       " 'blasted',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'blather',\n",
       " 'bleak',\n",
       " 'bleakly',\n",
       " 'bleakness',\n",
       " 'bleed',\n",
       " 'bleeding',\n",
       " 'bleeds',\n",
       " 'blemish',\n",
       " 'blind',\n",
       " 'blinding',\n",
       " 'blindingly',\n",
       " 'blindside',\n",
       " 'blister',\n",
       " 'blistering',\n",
       " 'bloated',\n",
       " 'blockage',\n",
       " 'blockhead',\n",
       " 'bloodshed',\n",
       " 'bloodthirsty',\n",
       " 'bloody',\n",
       " 'blotchy',\n",
       " 'blow',\n",
       " 'blunder',\n",
       " 'blundering',\n",
       " 'blunders',\n",
       " 'blunt',\n",
       " 'blur',\n",
       " 'bluring',\n",
       " 'blurred',\n",
       " 'blurring',\n",
       " 'blurry',\n",
       " 'blurs',\n",
       " 'blurt',\n",
       " 'boastful',\n",
       " 'boggle',\n",
       " 'bogus',\n",
       " 'boil',\n",
       " 'boiling',\n",
       " 'boisterous',\n",
       " 'bomb',\n",
       " 'bombard',\n",
       " 'bombardment',\n",
       " 'bombastic',\n",
       " 'bondage',\n",
       " 'bonkers',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boredom',\n",
       " 'bores',\n",
       " 'boring',\n",
       " 'botch',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bothering',\n",
       " 'bothers',\n",
       " 'bothersome',\n",
       " 'bowdlerize',\n",
       " 'boycott',\n",
       " 'braggart',\n",
       " 'bragger',\n",
       " 'brainless',\n",
       " 'brainwash',\n",
       " 'brash',\n",
       " 'brashly',\n",
       " 'brashness',\n",
       " 'brat',\n",
       " 'bravado',\n",
       " 'brazen',\n",
       " 'brazenly',\n",
       " 'brazenness',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'break-up',\n",
       " 'break-ups',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breakup',\n",
       " 'breakups',\n",
       " 'bribery',\n",
       " 'brimstone',\n",
       " 'bristle',\n",
       " 'brittle',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'broken-hearted',\n",
       " 'brood',\n",
       " 'browbeat',\n",
       " 'bruise',\n",
       " 'bruised',\n",
       " 'bruises',\n",
       " 'bruising',\n",
       " 'brusque',\n",
       " 'brutal',\n",
       " 'brutalising',\n",
       " 'brutalities',\n",
       " 'brutality',\n",
       " 'brutalize',\n",
       " 'brutalizing',\n",
       " 'brutally',\n",
       " 'brute',\n",
       " 'brutish',\n",
       " 'bs',\n",
       " 'buckle',\n",
       " 'bug',\n",
       " 'bugging',\n",
       " 'buggy',\n",
       " 'bugs',\n",
       " 'bulkier',\n",
       " 'bulkiness',\n",
       " 'bulky',\n",
       " 'bulkyness',\n",
       " 'bull****',\n",
       " 'bull----',\n",
       " 'bullies',\n",
       " 'bullshit',\n",
       " 'bullshyt',\n",
       " 'bully',\n",
       " 'bullying',\n",
       " 'bullyingly',\n",
       " 'bum',\n",
       " 'bump',\n",
       " 'bumped',\n",
       " 'bumping',\n",
       " 'bumpping',\n",
       " 'bumps',\n",
       " 'bumpy',\n",
       " 'bungle',\n",
       " 'bungler',\n",
       " 'bungling',\n",
       " 'bunk',\n",
       " 'burden',\n",
       " 'burdensome',\n",
       " 'burdensomely',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'bust',\n",
       " 'busts',\n",
       " 'busybody',\n",
       " 'butcher',\n",
       " 'butchery',\n",
       " 'buzzing',\n",
       " 'byzantine',\n",
       " 'cackle',\n",
       " 'calamities',\n",
       " 'calamitous',\n",
       " 'calamitously',\n",
       " 'calamity',\n",
       " 'callous',\n",
       " 'calumniate',\n",
       " 'calumniation',\n",
       " 'calumnies',\n",
       " 'calumnious',\n",
       " 'calumniously',\n",
       " 'calumny',\n",
       " 'cancer',\n",
       " 'cancerous',\n",
       " 'cannibal',\n",
       " 'cannibalize',\n",
       " 'capitulate',\n",
       " 'capricious',\n",
       " 'capriciously',\n",
       " 'capriciousness',\n",
       " 'capsize',\n",
       " 'careless',\n",
       " 'carelessness',\n",
       " 'caricature',\n",
       " 'carnage',\n",
       " 'carp',\n",
       " 'cartoonish',\n",
       " 'cash-strapped',\n",
       " 'castigate',\n",
       " 'castrated',\n",
       " 'casualty',\n",
       " 'cataclysm',\n",
       " 'cataclysmal',\n",
       " 'cataclysmic',\n",
       " 'cataclysmically',\n",
       " 'catastrophe',\n",
       " 'catastrophes',\n",
       " 'catastrophic',\n",
       " 'catastrophically',\n",
       " 'catastrophies',\n",
       " 'caustic',\n",
       " 'caustically',\n",
       " 'cautionary',\n",
       " 'cave',\n",
       " 'censure',\n",
       " 'chafe',\n",
       " 'chaff',\n",
       " 'chagrin',\n",
       " 'challenging',\n",
       " 'chaos',\n",
       " 'chaotic',\n",
       " 'chasten',\n",
       " 'chastise',\n",
       " 'chastisement',\n",
       " 'chatter',\n",
       " 'chatterbox',\n",
       " 'cheap',\n",
       " 'cheapen',\n",
       " 'cheaply',\n",
       " 'cheat',\n",
       " 'cheated',\n",
       " 'cheater',\n",
       " 'cheating',\n",
       " 'cheats',\n",
       " 'checkered',\n",
       " 'cheerless',\n",
       " 'cheesy',\n",
       " 'chide',\n",
       " 'childish',\n",
       " 'chill',\n",
       " 'chilly',\n",
       " 'chintzy',\n",
       " 'choke',\n",
       " 'choleric',\n",
       " 'choppy',\n",
       " 'chore',\n",
       " 'chronic',\n",
       " 'chunky',\n",
       " 'clamor',\n",
       " 'clamorous',\n",
       " 'clash',\n",
       " 'cliche',\n",
       " 'cliched',\n",
       " 'clique',\n",
       " 'clog',\n",
       " 'clogged',\n",
       " 'clogs',\n",
       " 'cloud',\n",
       " 'clouding',\n",
       " 'cloudy',\n",
       " 'clueless',\n",
       " 'clumsy',\n",
       " 'clunky',\n",
       " 'coarse',\n",
       " 'cocky',\n",
       " 'coerce',\n",
       " 'coercion',\n",
       " 'coercive',\n",
       " 'cold',\n",
       " 'coldly',\n",
       " 'collapse',\n",
       " 'collude',\n",
       " 'collusion',\n",
       " 'combative',\n",
       " 'combust',\n",
       " 'comical',\n",
       " 'commiserate',\n",
       " 'commonplace',\n",
       " 'commotion',\n",
       " 'commotions',\n",
       " 'complacent',\n",
       " 'complain',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'complains',\n",
       " 'complaint',\n",
       " 'complaints',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'complication',\n",
       " 'complicit',\n",
       " 'compulsion',\n",
       " 'compulsive',\n",
       " 'concede',\n",
       " 'conceded',\n",
       " 'conceit',\n",
       " 'conceited',\n",
       " 'concen',\n",
       " 'concens',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerns',\n",
       " 'concession',\n",
       " 'concessions',\n",
       " 'condemn',\n",
       " 'condemnable',\n",
       " 'condemnation',\n",
       " 'condemned',\n",
       " 'condemns',\n",
       " 'condescend',\n",
       " 'condescending',\n",
       " 'condescendingly',\n",
       " 'condescension',\n",
       " 'confess',\n",
       " 'confession',\n",
       " 'confessions',\n",
       " 'confined',\n",
       " 'conflict',\n",
       " 'conflicted',\n",
       " 'conflicting',\n",
       " 'conflicts',\n",
       " 'confound',\n",
       " 'confounded',\n",
       " 'confounding',\n",
       " 'confront',\n",
       " 'confrontation',\n",
       " 'confrontational',\n",
       " 'confuse',\n",
       " 'confused',\n",
       " 'confuses',\n",
       " 'confusing',\n",
       " 'confusion',\n",
       " 'confusions',\n",
       " 'congested',\n",
       " 'congestion',\n",
       " 'cons',\n",
       " 'conscons',\n",
       " 'conservative',\n",
       " 'conspicuous',\n",
       " 'conspicuously',\n",
       " 'conspiracies',\n",
       " 'conspiracy',\n",
       " 'conspirator',\n",
       " 'conspiratorial',\n",
       " 'conspire',\n",
       " 'consternation',\n",
       " 'contagious',\n",
       " 'contaminate',\n",
       " 'contaminated',\n",
       " 'contaminates',\n",
       " 'contaminating',\n",
       " 'contamination',\n",
       " 'contempt',\n",
       " 'contemptible',\n",
       " 'contemptuous',\n",
       " 'contemptuously',\n",
       " 'contend',\n",
       " 'contention',\n",
       " 'contentious',\n",
       " 'contort',\n",
       " 'contortions',\n",
       " 'contradict',\n",
       " 'contradiction',\n",
       " 'contradictory',\n",
       " 'contrariness',\n",
       " 'contravene',\n",
       " 'contrive',\n",
       " 'contrived',\n",
       " 'controversial',\n",
       " 'controversy',\n",
       " 'convoluted',\n",
       " 'corrode',\n",
       " 'corrosion',\n",
       " 'corrosions',\n",
       " 'corrosive',\n",
       " 'corrupt',\n",
       " 'corrupted',\n",
       " 'corrupting',\n",
       " 'corruption',\n",
       " 'corrupts',\n",
       " 'corruptted',\n",
       " 'costlier',\n",
       " 'costly',\n",
       " 'counter-productive',\n",
       " 'counterproductive',\n",
       " 'coupists',\n",
       " 'covetous',\n",
       " 'coward',\n",
       " 'cowardly',\n",
       " 'crabby',\n",
       " 'crack',\n",
       " 'cracked',\n",
       " 'cracks',\n",
       " 'craftily',\n",
       " 'craftly',\n",
       " 'crafty',\n",
       " 'cramp',\n",
       " 'cramped',\n",
       " 'cramping',\n",
       " 'cranky',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'craps',\n",
       " 'crash',\n",
       " 'crashed',\n",
       " 'crashes',\n",
       " 'crashing',\n",
       " 'crass',\n",
       " 'craven',\n",
       " 'cravenly',\n",
       " 'craze',\n",
       " 'crazily',\n",
       " 'craziness',\n",
       " 'crazy',\n",
       " 'creak',\n",
       " 'creaking',\n",
       " 'creaks',\n",
       " 'credulous',\n",
       " 'creep',\n",
       " 'creeping',\n",
       " 'creeps',\n",
       " 'creepy',\n",
       " 'crept',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'cringe',\n",
       " 'cringed',\n",
       " 'cringes',\n",
       " 'cripple',\n",
       " 'crippled',\n",
       " 'cripples',\n",
       " 'crippling',\n",
       " 'crisis',\n",
       " 'critic',\n",
       " 'critical',\n",
       " 'criticism',\n",
       " 'criticisms',\n",
       " 'criticize',\n",
       " 'criticized',\n",
       " 'criticizing',\n",
       " 'critics',\n",
       " 'cronyism',\n",
       " 'crook',\n",
       " 'crooked',\n",
       " 'crooks',\n",
       " 'crowded',\n",
       " 'crowdedness',\n",
       " 'crude',\n",
       " 'cruel',\n",
       " 'crueler',\n",
       " 'cruelest',\n",
       " 'cruelly',\n",
       " 'cruelness',\n",
       " 'cruelties',\n",
       " 'cruelty',\n",
       " 'crumble',\n",
       " 'crumbling',\n",
       " 'crummy',\n",
       " 'crumple',\n",
       " 'crumpled',\n",
       " 'crumples',\n",
       " 'crush',\n",
       " 'crushed',\n",
       " 'crushing',\n",
       " 'cry',\n",
       " 'culpable',\n",
       " 'culprit',\n",
       " 'cumbersome',\n",
       " 'cunt',\n",
       " 'cunts',\n",
       " 'cuplrit',\n",
       " 'curse',\n",
       " 'cursed',\n",
       " 'curses',\n",
       " 'curt',\n",
       " 'cuss',\n",
       " 'cussed',\n",
       " 'cutthroat',\n",
       " 'cynical',\n",
       " 'cynicism',\n",
       " 'd*mn',\n",
       " 'damage',\n",
       " 'damaged',\n",
       " 'damages',\n",
       " 'damaging',\n",
       " 'damn',\n",
       " 'damnable',\n",
       " 'damnably',\n",
       " 'damnation',\n",
       " 'damned',\n",
       " 'damning',\n",
       " 'damper',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'dangerousness',\n",
       " 'dark',\n",
       " 'darken',\n",
       " 'darkened',\n",
       " 'darker',\n",
       " 'darkness',\n",
       " 'dastard',\n",
       " 'dastardly',\n",
       " 'daunt',\n",
       " 'daunting',\n",
       " 'dauntingly',\n",
       " 'dawdle',\n",
       " 'daze',\n",
       " 'dazed',\n",
       " 'dead',\n",
       " 'deadbeat',\n",
       " 'deadlock',\n",
       " 'deadly',\n",
       " 'deadweight',\n",
       " 'deaf',\n",
       " 'dearth',\n",
       " 'death',\n",
       " 'debacle',\n",
       " 'debase',\n",
       " 'debasement',\n",
       " 'debaser',\n",
       " 'debatable',\n",
       " 'debauch',\n",
       " 'debaucher',\n",
       " 'debauchery',\n",
       " 'debilitate',\n",
       " 'debilitating',\n",
       " 'debility',\n",
       " 'debt',\n",
       " 'debts',\n",
       " 'decadence',\n",
       " 'decadent',\n",
       " 'decay',\n",
       " 'decayed',\n",
       " 'deceit',\n",
       " 'deceitful',\n",
       " 'deceitfully',\n",
       " 'deceitfulness',\n",
       " 'deceive',\n",
       " 'deceiver',\n",
       " 'deceivers',\n",
       " 'deceiving',\n",
       " 'deception',\n",
       " 'deceptive',\n",
       " 'deceptively',\n",
       " 'declaim',\n",
       " 'decline',\n",
       " 'declines',\n",
       " 'declining',\n",
       " 'decrement',\n",
       " 'decrepit',\n",
       " 'decrepitude',\n",
       " 'decry',\n",
       " 'defamation',\n",
       " 'defamations',\n",
       " 'defamatory',\n",
       " 'defame',\n",
       " 'defect',\n",
       " 'defective',\n",
       " 'defects',\n",
       " 'defensive',\n",
       " 'defiance',\n",
       " 'defiant',\n",
       " 'defiantly',\n",
       " 'deficiencies',\n",
       " 'deficiency',\n",
       " 'deficient',\n",
       " 'defile',\n",
       " 'defiler',\n",
       " 'deform',\n",
       " 'deformed',\n",
       " 'defrauding',\n",
       " 'defunct',\n",
       " 'defy',\n",
       " 'degenerate',\n",
       " 'degenerately',\n",
       " 'degeneration',\n",
       " 'degradation',\n",
       " 'degrade',\n",
       " 'degrading',\n",
       " 'degradingly',\n",
       " 'dehumanization',\n",
       " 'dehumanize',\n",
       " 'deign',\n",
       " 'deject',\n",
       " 'dejected',\n",
       " 'dejectedly',\n",
       " 'dejection',\n",
       " 'delay',\n",
       " 'delayed',\n",
       " 'delaying',\n",
       " 'delays',\n",
       " 'delinquency',\n",
       " 'delinquent',\n",
       " 'delirious',\n",
       " 'delirium',\n",
       " 'delude',\n",
       " 'deluded',\n",
       " 'deluge',\n",
       " 'delusion',\n",
       " 'delusional',\n",
       " 'delusions',\n",
       " 'demean',\n",
       " 'demeaning',\n",
       " 'demise',\n",
       " 'demolish',\n",
       " 'demolisher',\n",
       " 'demon',\n",
       " 'demonic',\n",
       " 'demonize',\n",
       " 'demonized',\n",
       " 'demonizes',\n",
       " 'demonizing',\n",
       " 'demoralize',\n",
       " 'demoralizing',\n",
       " 'demoralizingly',\n",
       " 'denial',\n",
       " 'denied',\n",
       " 'denies',\n",
       " 'denigrate',\n",
       " 'denounce',\n",
       " 'dense',\n",
       " 'dent',\n",
       " 'dented',\n",
       " 'dents',\n",
       " 'denunciate',\n",
       " 'denunciation',\n",
       " 'denunciations',\n",
       " 'deny',\n",
       " 'denying',\n",
       " 'deplete',\n",
       " 'deplorable',\n",
       " 'deplorably',\n",
       " 'deplore',\n",
       " 'deploring',\n",
       " 'deploringly',\n",
       " 'deprave',\n",
       " 'depraved',\n",
       " 'depravedly',\n",
       " 'deprecate',\n",
       " 'depress',\n",
       " 'depressed',\n",
       " 'depressing',\n",
       " 'depressingly',\n",
       " 'depression',\n",
       " 'depressions',\n",
       " 'deprive',\n",
       " 'deprived',\n",
       " 'deride',\n",
       " 'derision',\n",
       " 'derisive',\n",
       " 'derisively',\n",
       " 'derisiveness',\n",
       " 'derogatory',\n",
       " 'desecrate',\n",
       " 'desert',\n",
       " 'desertion',\n",
       " 'desiccate',\n",
       " 'desiccated',\n",
       " 'desititute',\n",
       " 'desolate',\n",
       " 'desolately',\n",
       " 'desolation',\n",
       " 'despair',\n",
       " 'despairing',\n",
       " 'despairingly',\n",
       " 'desperate',\n",
       " 'desperately',\n",
       " 'desperation',\n",
       " 'despicable',\n",
       " 'despicably',\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78ad53c0-2553-4f8d-ab5e-6dc0bba4a650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+',\n",
       " 'abound',\n",
       " 'abounds',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'accessable',\n",
       " 'accessible',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclamation',\n",
       " 'accolade',\n",
       " 'accolades',\n",
       " 'accommodative',\n",
       " 'accomodative',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'achievable',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achievible',\n",
       " 'acumen',\n",
       " 'adaptable',\n",
       " 'adaptive',\n",
       " 'adequate',\n",
       " 'adjustable',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admirer',\n",
       " 'admiring',\n",
       " 'admiringly',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adorer',\n",
       " 'adoring',\n",
       " 'adoringly',\n",
       " 'adroit',\n",
       " 'adroitly',\n",
       " 'adulate',\n",
       " 'adulation',\n",
       " 'adulatory',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advantageous',\n",
       " 'advantageously',\n",
       " 'advantages',\n",
       " 'adventuresome',\n",
       " 'adventurous',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'advocates',\n",
       " 'affability',\n",
       " 'affable',\n",
       " 'affably',\n",
       " 'affectation',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affinity',\n",
       " 'affirm',\n",
       " 'affirmation',\n",
       " 'affirmative',\n",
       " 'affluence',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'affordably',\n",
       " 'afordable',\n",
       " 'agile',\n",
       " 'agilely',\n",
       " 'agility',\n",
       " 'agreeable',\n",
       " 'agreeableness',\n",
       " 'agreeably',\n",
       " 'all-around',\n",
       " 'alluring',\n",
       " 'alluringly',\n",
       " 'altruistic',\n",
       " 'altruistically',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazement',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'ambitious',\n",
       " 'ambitiously',\n",
       " 'ameliorate',\n",
       " 'amenable',\n",
       " 'amenity',\n",
       " 'amiability',\n",
       " 'amiabily',\n",
       " 'amiable',\n",
       " 'amicability',\n",
       " 'amicable',\n",
       " 'amicably',\n",
       " 'amity',\n",
       " 'ample',\n",
       " 'amply',\n",
       " 'amuse',\n",
       " 'amusing',\n",
       " 'amusingly',\n",
       " 'angel',\n",
       " 'angelic',\n",
       " 'apotheosis',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'applaud',\n",
       " 'appreciable',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'appreciative',\n",
       " 'appreciatively',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'ardent',\n",
       " 'ardently',\n",
       " 'ardor',\n",
       " 'articulate',\n",
       " 'aspiration',\n",
       " 'aspirations',\n",
       " 'aspire',\n",
       " 'assurance',\n",
       " 'assurances',\n",
       " 'assure',\n",
       " 'assuredly',\n",
       " 'assuring',\n",
       " 'astonish',\n",
       " 'astonished',\n",
       " 'astonishing',\n",
       " 'astonishingly',\n",
       " 'astonishment',\n",
       " 'astound',\n",
       " 'astounded',\n",
       " 'astounding',\n",
       " 'astoundingly',\n",
       " 'astutely',\n",
       " 'attentive',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'attractively',\n",
       " 'attune',\n",
       " 'audible',\n",
       " 'audibly',\n",
       " 'auspicious',\n",
       " 'authentic',\n",
       " 'authoritative',\n",
       " 'autonomous',\n",
       " 'available',\n",
       " 'aver',\n",
       " 'avid',\n",
       " 'avidly',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'awards',\n",
       " 'awe',\n",
       " 'awed',\n",
       " 'awesome',\n",
       " 'awesomely',\n",
       " 'awesomeness',\n",
       " 'awestruck',\n",
       " 'awsome',\n",
       " 'backbone',\n",
       " 'balanced',\n",
       " 'bargain',\n",
       " 'beauteous',\n",
       " 'beautiful',\n",
       " 'beautifullly',\n",
       " 'beautifully',\n",
       " 'beautify',\n",
       " 'beauty',\n",
       " 'beckon',\n",
       " 'beckoned',\n",
       " 'beckoning',\n",
       " 'beckons',\n",
       " 'believable',\n",
       " 'believeable',\n",
       " 'beloved',\n",
       " 'benefactor',\n",
       " 'beneficent',\n",
       " 'beneficial',\n",
       " 'beneficially',\n",
       " 'beneficiary',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'benevolence',\n",
       " 'benevolent',\n",
       " 'benifits',\n",
       " 'best',\n",
       " 'best-known',\n",
       " 'best-performing',\n",
       " 'best-selling',\n",
       " 'better',\n",
       " 'better-known',\n",
       " 'better-than-expected',\n",
       " 'beutifully',\n",
       " 'blameless',\n",
       " 'bless',\n",
       " 'blessing',\n",
       " 'bliss',\n",
       " 'blissful',\n",
       " 'blissfully',\n",
       " 'blithe',\n",
       " 'blockbuster',\n",
       " 'bloom',\n",
       " 'blossom',\n",
       " 'bolster',\n",
       " 'bonny',\n",
       " 'bonus',\n",
       " 'bonuses',\n",
       " 'boom',\n",
       " 'booming',\n",
       " 'boost',\n",
       " 'boundless',\n",
       " 'bountiful',\n",
       " 'brainiest',\n",
       " 'brainy',\n",
       " 'brand-new',\n",
       " 'brave',\n",
       " 'bravery',\n",
       " 'bravo',\n",
       " 'breakthrough',\n",
       " 'breakthroughs',\n",
       " 'breathlessness',\n",
       " 'breathtaking',\n",
       " 'breathtakingly',\n",
       " 'breeze',\n",
       " 'bright',\n",
       " 'brighten',\n",
       " 'brighter',\n",
       " 'brightest',\n",
       " 'brilliance',\n",
       " 'brilliances',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'brisk',\n",
       " 'brotherly',\n",
       " 'bullish',\n",
       " 'buoyant',\n",
       " 'cajole',\n",
       " 'calm',\n",
       " 'calming',\n",
       " 'calmness',\n",
       " 'capability',\n",
       " 'capable',\n",
       " 'capably',\n",
       " 'captivate',\n",
       " 'captivating',\n",
       " 'carefree',\n",
       " 'cashback',\n",
       " 'cashbacks',\n",
       " 'catchy',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'celebration',\n",
       " 'celebratory',\n",
       " 'champ',\n",
       " 'champion',\n",
       " 'charisma',\n",
       " 'charismatic',\n",
       " 'charitable',\n",
       " 'charm',\n",
       " 'charming',\n",
       " 'charmingly',\n",
       " 'chaste',\n",
       " 'cheaper',\n",
       " 'cheapest',\n",
       " 'cheer',\n",
       " 'cheerful',\n",
       " 'cheery',\n",
       " 'cherish',\n",
       " 'cherished',\n",
       " 'cherub',\n",
       " 'chic',\n",
       " 'chivalrous',\n",
       " 'chivalry',\n",
       " 'civility',\n",
       " 'civilize',\n",
       " 'clarity',\n",
       " 'classic',\n",
       " 'classy',\n",
       " 'clean',\n",
       " 'cleaner',\n",
       " 'cleanest',\n",
       " 'cleanliness',\n",
       " 'cleanly',\n",
       " 'clear',\n",
       " 'clear-cut',\n",
       " 'cleared',\n",
       " 'clearer',\n",
       " 'clearly',\n",
       " 'clears',\n",
       " 'clever',\n",
       " 'cleverly',\n",
       " 'cohere',\n",
       " 'coherence',\n",
       " 'coherent',\n",
       " 'cohesive',\n",
       " 'colorful',\n",
       " 'comely',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comfortably',\n",
       " 'comforting',\n",
       " 'comfy',\n",
       " 'commend',\n",
       " 'commendable',\n",
       " 'commendably',\n",
       " 'commitment',\n",
       " 'commodious',\n",
       " 'compact',\n",
       " 'compactly',\n",
       " 'compassion',\n",
       " 'compassionate',\n",
       " 'compatible',\n",
       " 'competitive',\n",
       " 'complement',\n",
       " 'complementary',\n",
       " 'complemented',\n",
       " 'complements',\n",
       " 'compliant',\n",
       " 'compliment',\n",
       " 'complimentary',\n",
       " 'comprehensive',\n",
       " 'conciliate',\n",
       " 'conciliatory',\n",
       " 'concise',\n",
       " 'confidence',\n",
       " 'confident',\n",
       " 'congenial',\n",
       " 'congratulate',\n",
       " 'congratulation',\n",
       " 'congratulations',\n",
       " 'congratulatory',\n",
       " 'conscientious',\n",
       " 'considerate',\n",
       " 'consistent',\n",
       " 'consistently',\n",
       " 'constructive',\n",
       " 'consummate',\n",
       " 'contentment',\n",
       " 'continuity',\n",
       " 'contrasty',\n",
       " 'contribution',\n",
       " 'convenience',\n",
       " 'convenient',\n",
       " 'conveniently',\n",
       " 'convience',\n",
       " 'convienient',\n",
       " 'convient',\n",
       " 'convincing',\n",
       " 'convincingly',\n",
       " 'cool',\n",
       " 'coolest',\n",
       " 'cooperative',\n",
       " 'cooperatively',\n",
       " 'cornerstone',\n",
       " 'correct',\n",
       " 'correctly',\n",
       " 'cost-effective',\n",
       " 'cost-saving',\n",
       " 'counter-attack',\n",
       " 'counter-attacks',\n",
       " 'courage',\n",
       " 'courageous',\n",
       " 'courageously',\n",
       " 'courageousness',\n",
       " 'courteous',\n",
       " 'courtly',\n",
       " 'covenant',\n",
       " 'cozy',\n",
       " 'creative',\n",
       " 'credence',\n",
       " 'credible',\n",
       " 'crisp',\n",
       " 'crisper',\n",
       " 'cure',\n",
       " 'cure-all',\n",
       " 'cushy',\n",
       " 'cute',\n",
       " 'cuteness',\n",
       " 'danke',\n",
       " 'danken',\n",
       " 'daring',\n",
       " 'daringly',\n",
       " 'darling',\n",
       " 'dashing',\n",
       " 'dauntless',\n",
       " 'dawn',\n",
       " 'dazzle',\n",
       " 'dazzled',\n",
       " 'dazzling',\n",
       " 'dead-cheap',\n",
       " 'dead-on',\n",
       " 'decency',\n",
       " 'decent',\n",
       " 'decisive',\n",
       " 'decisiveness',\n",
       " 'dedicated',\n",
       " 'defeat',\n",
       " 'defeated',\n",
       " 'defeating',\n",
       " 'defeats',\n",
       " 'defender',\n",
       " 'deference',\n",
       " 'deft',\n",
       " 'deginified',\n",
       " 'delectable',\n",
       " 'delicacy',\n",
       " 'delicate',\n",
       " 'delicious',\n",
       " 'delight',\n",
       " 'delighted',\n",
       " 'delightful',\n",
       " 'delightfully',\n",
       " 'delightfulness',\n",
       " 'dependable',\n",
       " 'dependably',\n",
       " 'deservedly',\n",
       " 'deserving',\n",
       " 'desirable',\n",
       " 'desiring',\n",
       " 'desirous',\n",
       " 'destiny',\n",
       " 'detachable',\n",
       " 'devout',\n",
       " 'dexterous',\n",
       " 'dexterously',\n",
       " 'dextrous',\n",
       " 'dignified',\n",
       " 'dignify',\n",
       " 'dignity',\n",
       " 'diligence',\n",
       " 'diligent',\n",
       " 'diligently',\n",
       " 'diplomatic',\n",
       " 'dirt-cheap',\n",
       " 'distinction',\n",
       " 'distinctive',\n",
       " 'distinguished',\n",
       " 'diversified',\n",
       " 'divine',\n",
       " 'divinely',\n",
       " 'dominate',\n",
       " 'dominated',\n",
       " 'dominates',\n",
       " 'dote',\n",
       " 'dotingly',\n",
       " 'doubtless',\n",
       " 'dreamland',\n",
       " 'dumbfounded',\n",
       " 'dumbfounding',\n",
       " 'dummy-proof',\n",
       " 'durable',\n",
       " 'dynamic',\n",
       " 'eager',\n",
       " 'eagerly',\n",
       " 'eagerness',\n",
       " 'earnest',\n",
       " 'earnestly',\n",
       " 'earnestness',\n",
       " 'ease',\n",
       " 'eased',\n",
       " 'eases',\n",
       " 'easier',\n",
       " 'easiest',\n",
       " 'easiness',\n",
       " 'easing',\n",
       " 'easy',\n",
       " 'easy-to-use',\n",
       " 'easygoing',\n",
       " 'ebullience',\n",
       " 'ebullient',\n",
       " 'ebulliently',\n",
       " 'ecenomical',\n",
       " 'economical',\n",
       " 'ecstasies',\n",
       " 'ecstasy',\n",
       " 'ecstatic',\n",
       " 'ecstatically',\n",
       " 'edify',\n",
       " 'educated',\n",
       " 'effective',\n",
       " 'effectively',\n",
       " 'effectiveness',\n",
       " 'effectual',\n",
       " 'efficacious',\n",
       " 'efficient',\n",
       " 'efficiently',\n",
       " 'effortless',\n",
       " 'effortlessly',\n",
       " 'effusion',\n",
       " 'effusive',\n",
       " 'effusively',\n",
       " 'effusiveness',\n",
       " 'elan',\n",
       " 'elate',\n",
       " 'elated',\n",
       " 'elatedly',\n",
       " 'elation',\n",
       " 'electrify',\n",
       " 'elegance',\n",
       " 'elegant',\n",
       " 'elegantly',\n",
       " 'elevate',\n",
       " 'elite',\n",
       " 'eloquence',\n",
       " 'eloquent',\n",
       " 'eloquently',\n",
       " 'embolden',\n",
       " 'eminence',\n",
       " 'eminent',\n",
       " 'empathize',\n",
       " 'empathy',\n",
       " 'empower',\n",
       " 'empowerment',\n",
       " 'enchant',\n",
       " 'enchanted',\n",
       " 'enchanting',\n",
       " 'enchantingly',\n",
       " 'encourage',\n",
       " 'encouragement',\n",
       " 'encouraging',\n",
       " 'encouragingly',\n",
       " 'endear',\n",
       " 'endearing',\n",
       " 'endorse',\n",
       " 'endorsed',\n",
       " 'endorsement',\n",
       " 'endorses',\n",
       " 'endorsing',\n",
       " 'energetic',\n",
       " 'energize',\n",
       " 'energy-efficient',\n",
       " 'energy-saving',\n",
       " 'engaging',\n",
       " 'engrossing',\n",
       " 'enhance',\n",
       " 'enhanced',\n",
       " 'enhancement',\n",
       " 'enhances',\n",
       " 'enjoy',\n",
       " 'enjoyable',\n",
       " 'enjoyably',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enjoyment',\n",
       " 'enjoys',\n",
       " 'enlighten',\n",
       " 'enlightenment',\n",
       " 'enliven',\n",
       " 'ennoble',\n",
       " 'enough',\n",
       " 'enrapt',\n",
       " 'enrapture',\n",
       " 'enraptured',\n",
       " 'enrich',\n",
       " 'enrichment',\n",
       " 'enterprising',\n",
       " 'entertain',\n",
       " 'entertaining',\n",
       " 'entertains',\n",
       " 'enthral',\n",
       " 'enthrall',\n",
       " 'enthralled',\n",
       " 'enthuse',\n",
       " 'enthusiasm',\n",
       " 'enthusiast',\n",
       " 'enthusiastic',\n",
       " 'enthusiastically',\n",
       " 'entice',\n",
       " 'enticed',\n",
       " 'enticing',\n",
       " 'enticingly',\n",
       " 'entranced',\n",
       " 'entrancing',\n",
       " 'entrust',\n",
       " 'enviable',\n",
       " 'enviably',\n",
       " 'envious',\n",
       " 'enviously',\n",
       " 'enviousness',\n",
       " 'envy',\n",
       " 'equitable',\n",
       " 'ergonomical',\n",
       " 'err-free',\n",
       " 'erudite',\n",
       " 'ethical',\n",
       " 'eulogize',\n",
       " 'euphoria',\n",
       " 'euphoric',\n",
       " 'euphorically',\n",
       " 'evaluative',\n",
       " 'evenly',\n",
       " 'eventful',\n",
       " 'everlasting',\n",
       " 'evocative',\n",
       " 'exalt',\n",
       " 'exaltation',\n",
       " 'exalted',\n",
       " 'exaltedly',\n",
       " 'exalting',\n",
       " 'exaltingly',\n",
       " 'examplar',\n",
       " 'examplary',\n",
       " 'excallent',\n",
       " 'exceed',\n",
       " 'exceeded',\n",
       " 'exceeding',\n",
       " 'exceedingly',\n",
       " 'exceeds',\n",
       " 'excel',\n",
       " 'exceled',\n",
       " 'excelent',\n",
       " 'excellant',\n",
       " 'excelled',\n",
       " 'excellence',\n",
       " 'excellency',\n",
       " 'excellent',\n",
       " 'excellently',\n",
       " 'excels',\n",
       " 'exceptional',\n",
       " 'exceptionally',\n",
       " 'excite',\n",
       " 'excited',\n",
       " 'excitedly',\n",
       " 'excitedness',\n",
       " 'excitement',\n",
       " 'excites',\n",
       " 'exciting',\n",
       " 'excitingly',\n",
       " 'exellent',\n",
       " 'exemplar',\n",
       " 'exemplary',\n",
       " 'exhilarate',\n",
       " 'exhilarating',\n",
       " 'exhilaratingly',\n",
       " 'exhilaration',\n",
       " 'exonerate',\n",
       " 'expansive',\n",
       " 'expeditiously',\n",
       " 'expertly',\n",
       " 'exquisite',\n",
       " 'exquisitely',\n",
       " 'extol',\n",
       " 'extoll',\n",
       " 'extraordinarily',\n",
       " 'extraordinary',\n",
       " 'exuberance',\n",
       " 'exuberant',\n",
       " 'exuberantly',\n",
       " 'exult',\n",
       " 'exultant',\n",
       " 'exultation',\n",
       " 'exultingly',\n",
       " 'eye-catch',\n",
       " 'eye-catching',\n",
       " 'eyecatch',\n",
       " 'eyecatching',\n",
       " 'fabulous',\n",
       " 'fabulously',\n",
       " 'facilitate',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fairness',\n",
       " 'faith',\n",
       " 'faithful',\n",
       " 'faithfully',\n",
       " 'faithfulness',\n",
       " 'fame',\n",
       " 'famed',\n",
       " 'famous',\n",
       " 'famously',\n",
       " 'fancier',\n",
       " 'fancinating',\n",
       " 'fancy',\n",
       " 'fanfare',\n",
       " 'fans',\n",
       " 'fantastic',\n",
       " 'fantastically',\n",
       " 'fascinate',\n",
       " 'fascinating',\n",
       " 'fascinatingly',\n",
       " 'fascination',\n",
       " 'fashionable',\n",
       " 'fashionably',\n",
       " 'fast',\n",
       " 'fast-growing',\n",
       " 'fast-paced',\n",
       " 'faster',\n",
       " 'fastest',\n",
       " 'fastest-growing',\n",
       " 'faultless',\n",
       " 'fav',\n",
       " 'fave',\n",
       " 'favor',\n",
       " 'favorable',\n",
       " 'favored',\n",
       " 'favorite',\n",
       " 'favorited',\n",
       " 'favour',\n",
       " 'fearless',\n",
       " 'fearlessly',\n",
       " 'feasible',\n",
       " 'feasibly',\n",
       " 'feat',\n",
       " 'feature-rich',\n",
       " 'fecilitous',\n",
       " 'feisty',\n",
       " 'felicitate',\n",
       " 'felicitous',\n",
       " 'felicity',\n",
       " 'fertile',\n",
       " 'fervent',\n",
       " 'fervently',\n",
       " 'fervid',\n",
       " 'fervidly',\n",
       " 'fervor',\n",
       " 'festive',\n",
       " 'fidelity',\n",
       " 'fiery',\n",
       " 'fine',\n",
       " 'fine-looking',\n",
       " 'finely',\n",
       " 'finer',\n",
       " 'finest',\n",
       " 'firmer',\n",
       " 'first-class',\n",
       " 'first-in-class',\n",
       " 'first-rate',\n",
       " 'flashy',\n",
       " 'flatter',\n",
       " 'flattering',\n",
       " 'flatteringly',\n",
       " 'flawless',\n",
       " 'flawlessly',\n",
       " 'flexibility',\n",
       " 'flexible',\n",
       " 'flourish',\n",
       " 'flourishing',\n",
       " 'fluent',\n",
       " 'flutter',\n",
       " 'fond',\n",
       " 'fondly',\n",
       " 'fondness',\n",
       " 'foolproof',\n",
       " 'foremost',\n",
       " 'foresight',\n",
       " 'formidable',\n",
       " 'fortitude',\n",
       " 'fortuitous',\n",
       " 'fortuitously',\n",
       " 'fortunate',\n",
       " 'fortunately',\n",
       " 'fortune',\n",
       " 'fragrant',\n",
       " 'free',\n",
       " 'freed',\n",
       " 'freedom',\n",
       " 'freedoms',\n",
       " 'fresh',\n",
       " 'fresher',\n",
       " 'freshest',\n",
       " 'friendliness',\n",
       " 'friendly',\n",
       " 'frolic',\n",
       " 'frugal',\n",
       " 'fruitful',\n",
       " 'ftw',\n",
       " 'fulfillment',\n",
       " 'fun',\n",
       " 'futurestic',\n",
       " 'futuristic',\n",
       " 'gaiety',\n",
       " 'gaily',\n",
       " 'gain',\n",
       " 'gained',\n",
       " 'gainful',\n",
       " 'gainfully',\n",
       " 'gaining',\n",
       " 'gains',\n",
       " 'gallant',\n",
       " 'gallantly',\n",
       " 'galore',\n",
       " 'geekier',\n",
       " 'geeky',\n",
       " 'gem',\n",
       " 'gems',\n",
       " 'generosity',\n",
       " 'generous',\n",
       " 'generously',\n",
       " 'genial',\n",
       " 'genius',\n",
       " 'gentle',\n",
       " 'gentlest',\n",
       " 'genuine',\n",
       " 'gifted',\n",
       " 'glad',\n",
       " 'gladden',\n",
       " 'gladly',\n",
       " 'gladness',\n",
       " 'glamorous',\n",
       " 'glee',\n",
       " 'gleeful',\n",
       " 'gleefully',\n",
       " 'glimmer',\n",
       " 'glimmering',\n",
       " 'glisten',\n",
       " 'glistening',\n",
       " 'glitter',\n",
       " 'glitz',\n",
       " 'glorify',\n",
       " 'glorious',\n",
       " 'gloriously',\n",
       " 'glory',\n",
       " 'glow',\n",
       " 'glowing',\n",
       " 'glowingly',\n",
       " 'god-given',\n",
       " 'god-send',\n",
       " 'godlike',\n",
       " 'godsend',\n",
       " 'gold',\n",
       " 'golden',\n",
       " 'good',\n",
       " 'goodly',\n",
       " 'goodness',\n",
       " 'goodwill',\n",
       " 'goood',\n",
       " 'gooood',\n",
       " 'gorgeous',\n",
       " 'gorgeously',\n",
       " 'grace',\n",
       " 'graceful',\n",
       " 'gracefully',\n",
       " 'gracious',\n",
       " 'graciously',\n",
       " 'graciousness',\n",
       " 'grand',\n",
       " 'grandeur',\n",
       " 'grateful',\n",
       " 'gratefully',\n",
       " 'gratification',\n",
       " 'gratified',\n",
       " 'gratifies',\n",
       " 'gratify',\n",
       " 'gratifying',\n",
       " 'gratifyingly',\n",
       " 'gratitude',\n",
       " 'great',\n",
       " 'greatest',\n",
       " 'greatness',\n",
       " 'grin',\n",
       " 'groundbreaking',\n",
       " 'guarantee',\n",
       " 'guidance',\n",
       " 'guiltless',\n",
       " 'gumption',\n",
       " 'gush',\n",
       " 'gusto',\n",
       " 'gutsy',\n",
       " 'hail',\n",
       " 'halcyon',\n",
       " 'hale',\n",
       " 'hallmark',\n",
       " 'hallmarks',\n",
       " 'hallowed',\n",
       " 'handier',\n",
       " 'handily',\n",
       " 'hands-down',\n",
       " 'handsome',\n",
       " 'handsomely',\n",
       " 'handy',\n",
       " 'happier',\n",
       " 'happily',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard-working',\n",
       " 'hardier',\n",
       " 'hardy',\n",
       " 'harmless',\n",
       " 'harmonious',\n",
       " 'harmoniously',\n",
       " 'harmonize',\n",
       " 'harmony',\n",
       " 'headway',\n",
       " 'heal',\n",
       " 'healthful',\n",
       " 'healthy',\n",
       " 'hearten',\n",
       " 'heartening',\n",
       " 'heartfelt',\n",
       " 'heartily',\n",
       " 'heartwarming',\n",
       " 'heaven',\n",
       " 'heavenly',\n",
       " 'helped',\n",
       " 'helpful',\n",
       " 'helping',\n",
       " 'hero',\n",
       " 'heroic',\n",
       " 'heroically',\n",
       " 'heroine',\n",
       " 'heroize',\n",
       " 'heros',\n",
       " 'high-quality',\n",
       " 'high-spirited',\n",
       " 'hilarious',\n",
       " 'holy',\n",
       " 'homage',\n",
       " 'honest',\n",
       " 'honesty',\n",
       " 'honor',\n",
       " 'honorable',\n",
       " 'honored',\n",
       " 'honoring',\n",
       " 'hooray',\n",
       " 'hopeful',\n",
       " 'hospitable',\n",
       " 'hot',\n",
       " 'hotcake',\n",
       " 'hotcakes',\n",
       " 'hottest',\n",
       " 'hug',\n",
       " 'humane',\n",
       " 'humble',\n",
       " 'humility',\n",
       " 'humor',\n",
       " 'humorous',\n",
       " 'humorously',\n",
       " 'humour',\n",
       " 'humourous',\n",
       " 'ideal',\n",
       " 'idealize',\n",
       " 'ideally',\n",
       " 'idol',\n",
       " 'idolize',\n",
       " 'idolized',\n",
       " 'idyllic',\n",
       " 'illuminate',\n",
       " 'illuminati',\n",
       " 'illuminating',\n",
       " 'illumine',\n",
       " 'illustrious',\n",
       " 'ilu',\n",
       " 'imaculate',\n",
       " 'imaginative',\n",
       " 'immaculate',\n",
       " 'immaculately',\n",
       " 'immense',\n",
       " 'impartial',\n",
       " 'impartiality',\n",
       " 'impartially',\n",
       " 'impassioned',\n",
       " 'impeccable',\n",
       " 'impeccably',\n",
       " 'important',\n",
       " 'impress',\n",
       " 'impressed',\n",
       " 'impresses',\n",
       " 'impressive',\n",
       " 'impressively',\n",
       " 'impressiveness',\n",
       " 'improve',\n",
       " 'improved',\n",
       " 'improvement',\n",
       " 'improvements',\n",
       " 'improves',\n",
       " 'improving',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'indebted',\n",
       " 'individualized',\n",
       " 'indulgence',\n",
       " 'indulgent',\n",
       " 'industrious',\n",
       " 'inestimable',\n",
       " 'inestimably',\n",
       " 'inexpensive',\n",
       " 'infallibility',\n",
       " 'infallible',\n",
       " 'infallibly',\n",
       " 'influential',\n",
       " 'ingenious',\n",
       " 'ingeniously',\n",
       " 'ingenuity',\n",
       " 'ingenuous',\n",
       " 'ingenuously',\n",
       " 'innocuous',\n",
       " 'innovation',\n",
       " 'innovative',\n",
       " 'inpressed',\n",
       " 'insightful',\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a42f7-79c2-4ac0-b7a3-9f0bb99229e4",
   "metadata": {},
   "source": [
    "## 1) POSITIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35dda1be-72f4-41b0-aa0f-c2d4feb8944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postive score= 1\n"
     ]
    }
   ],
   "source": [
    "positive_score=0\n",
    "for i in tokenize_text:\n",
    "  if(i.lower() in post):\n",
    "    positive_score+=1\n",
    "print('postive score=', positive_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e3b59-8d3d-40be-a31f-4c1a9987d845",
   "metadata": {},
   "source": [
    "## 2)Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5ba8676-5a04-4207-b74a-b3140650b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative score= 0\n"
     ]
    }
   ],
   "source": [
    "negative_score=0\n",
    "for i in tokenize_text:\n",
    "  if(i.lower() in neg):\n",
    "    negative_score+=1\n",
    "print('negative score=', negative_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfd059-71a1-4c4a-abd4-42f0b161aa8a",
   "metadata": {},
   "source": [
    "## 3)Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d3552fe-d828-41c0-bf15-1f3e5b6fa936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity_score= 0.9999990000010001\n"
     ]
    }
   ],
   "source": [
    "#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
    "print('polarity_score=', Polarity_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf734824-2804-4b08-9215-df7a54b0637e",
   "metadata": {},
   "source": [
    "## 4)Subjective Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31625d69-ee74-4859-96ca-08d68d0c176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjectivity_score 0.08333332638888948\n"
     ]
    }
   ],
   "source": [
    "#Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)\n",
    "print('subjectivity_score',subjectiivity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49d875-3e21-4a46-91a9-4f5fdd327d3e",
   "metadata": {},
   "source": [
    "## 5)Avg sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f89466b5-65ea-486a-8a4c-bae20ae68b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg sentence length= 66.0\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "  avg_length.append(len(c['abc'].iloc[i]))\n",
    "avg_senetence_length=sum(avg_length)/len(avg_length)\n",
    "print('avg sentence length=', avg_senetence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc2faf-a1ff-4ca9-9ffb-2249e84f3e47",
   "metadata": {},
   "source": [
    "## 6) percentage of complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9787afe4-02f8-43f7-86f0-9c86ad9e2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "  if(count>2):\n",
    "   complex_Word_Count+=1\n",
    "  count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33e6cc7f-0933-4f65-ac2a-b1a624aa11b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentag of complex words=  0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "Percentage_of_Complex_words=complex_Word_Count/len(tokenize_text)\n",
    "print('percentag of complex words= ',Percentage_of_Complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe804822-cbb2-4df1-b767-4edd930e653a",
   "metadata": {},
   "source": [
    "## 7)fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30e66fbe-92a1-441f-8c0f-fc1438428ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fog index=  26.433333333333334\n"
     ]
    }
   ],
   "source": [
    "#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "Fog_Index = 0.4 * (avg_senetence_length + Percentage_of_Complex_words)\n",
    "print('fog index= ',Fog_Index )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec14eb0-3c4f-454b-be3e-13ac698b94c8",
   "metadata": {},
   "source": [
    "## 8)avg number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4169a458-35fa-4d14-a01c-d7a9ad42eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg no of words per sentence=  11.0\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "  a=[word.split( ) for word in c.iloc[i]]\n",
    "  avg_length.append(len(a[0]))\n",
    "  a=0\n",
    "#avg\n",
    "avg_no_of_words_per_sentence=sum(avg_length)/length\n",
    "print(\"avg no of words per sentence= \",avg_no_of_words_per_sentence)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa736fb-aa97-4b0b-98bb-85a092c7667a",
   "metadata": {},
   "source": [
    "## 9)complex word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4850f4d2-cab3-4df8-8a7a-a79599303966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex words count= 1\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "  if(count>2):\n",
    "   complex_Word_Count+=1\n",
    "  count=0\n",
    "print('complex words count=',  complex_Word_Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e1ee1-760d-441f-9e6c-803f83098163",
   "metadata": {},
   "source": [
    "## 10)word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34b0246e-6953-47d5-9b20-836635511e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count=  12\n"
     ]
    }
   ],
   "source": [
    "word_count=len(tokenize_text)\n",
    "print('word count= ', word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366018f-893b-40ee-95b3-6c7bb168e9a5",
   "metadata": {},
   "source": [
    "## 11) Syllable per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ebf50b91-0c36-4374-8c29-1b4f2a97df90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syllable_per_word=  15\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "syllable_count=count\n",
    "print('syllable_per_word= ',syllable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b95ad2-238c-4041-b0e6-eb011b2c0e4f",
   "metadata": {},
   "source": [
    "## 12) Personal pronouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20ec536d-6992-459a-9b70-2287bd0317c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal pronouns=  0\n"
     ]
    }
   ],
   "source": [
    "pronouns=['i','we','my','ours','us' ]\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "  if i.lower() in pronouns:\n",
    "   count+=1\n",
    "personal_pronouns=count\n",
    "print('personal pronouns= ',personal_pronouns )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb1f2e-19e6-4d0a-9e1c-43084749a164",
   "metadata": {},
   "source": [
    "## 13) Avg word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b68dc90-9e2c-4f1a-89db-11a841ed48c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg word=  4.666666666666667\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in tokenize_text:\n",
    "  for j in i:\n",
    "    count+=1\n",
    "avg_word_length=count/len(tokenize_text)\n",
    "print('avg word= ', avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc3b503f-d03c-4412-86dc-9a4e4bc50c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'positive_score':positive_score,'negative_score':negative_score,'Polarity_Score':Polarity_Score,'subjectiivity_score':subjectiivity_score,'avg_senetence_length':avg_senetence_length,'Percentage_of_Complex_words':Percentage_of_Complex_words,'Fog_Index':Fog_Index,'avg_no_of_words_per_sentence':avg_no_of_words_per_sentence,'complex_Word_Count':complex_Word_Count,'word_count':word_count,'syllable_count':syllable_count,'personal_pronouns':personal_pronouns,'avg_word_length':avg_word_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c55ddab6-5d65-4992-8a8e-99ec50b8fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  positive_score  \\\n",
      "0  This is an excellent example of a positive sen...             2.0   \n",
      "1  This is a terrible example of a negative sente...             0.0   \n",
      "\n",
      "   negative_score  polarity_score  subjectivity_score  avg_sentence_length  \\\n",
      "0             0.0             1.0            0.772727                 10.0   \n",
      "1             2.0            -1.0            0.700000                 10.0   \n",
      "\n",
      "   percentage_of_complex_words  fog_index  avg_no_of_words_per_sentence  \\\n",
      "0                         40.0       20.0                          10.0   \n",
      "1                         40.0       20.0                          10.0   \n",
      "\n",
      "   complex_word_count  word_count  syllable_count  personal_pronouns  \\\n",
      "0                 4.0        10.0            18.0                0.0   \n",
      "1                 4.0        10.0            18.0                0.0   \n",
      "\n",
      "   avg_word_length  \n",
      "0              4.4  \n",
      "1              4.2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.data.path.append('C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "nltk.download('punkt', download_dir='C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "nltk.download('stopwords', download_dir='C:/Users/USER/OneDrive/Desktop/BlackCoffer/Nltk_data')\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Load word lists\n",
    "def load_word_list(filepath):\n",
    "    with open(filepath, 'r', encoding='ISO-8859-1') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "positive = load_word_list('C:/Users/USER/OneDrive/Desktop/BlackCoffer/MasterDictionary/positive-words.txt')\n",
    "negative = load_word_list('C:/Users/USER/OneDrive/Desktop/BlackCoffer/MasterDictionary/negative-words.txt')\n",
    "\n",
    "# Functions to calculate variables\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiouy'\n",
    "    count = 0\n",
    "    previous_char_was_vowel = False\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            if not previous_char_was_vowel:\n",
    "                count += 1\n",
    "                previous_char_was_vowel = True\n",
    "        else:\n",
    "            previous_char_was_vowel = False\n",
    "    return count\n",
    "\n",
    "def calculate_variables(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    word_count = len(tokens)\n",
    "    positive_score = sum(1 for word in tokens if word.lower() in positive)\n",
    "    negative_score = sum(1 for word in tokens if word.lower() in negative)\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = TextBlob(text).sentiment.subjectivity\n",
    "    \n",
    "    avg_sentence_length = word_count / len(sentences) if sentences else 0\n",
    "    complex_word_count = sum(1 for word in tokens if count_syllables(word) >= 3)\n",
    "    percentage_of_complex_words = (complex_word_count / word_count) * 100 if word_count else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "    \n",
    "    syllable_count = sum(count_syllables(word) for word in tokens)\n",
    "    personal_pronouns = sum(1 for word in tokens if word.lower() in [\"i\", \"we\", \"my\", \"ours\", \"us\"])\n",
    "    avg_word_length = sum(len(word) for word in tokens) / word_count if word_count else 0\n",
    "    \n",
    "    return {\n",
    "        'positive_score': positive_score,\n",
    "        'negative_score': negative_score,\n",
    "        'polarity_score': polarity_score,\n",
    "        'subjectivity_score': subjectivity_score,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'percentage_of_complex_words': percentage_of_complex_words,\n",
    "        'fog_index': fog_index,\n",
    "        'avg_no_of_words_per_sentence': avg_sentence_length,\n",
    "        'complex_word_count': complex_word_count,\n",
    "        'word_count': word_count,\n",
    "        'syllable_count': syllable_count,\n",
    "        'personal_pronouns': personal_pronouns,\n",
    "        'avg_word_length': avg_word_length\n",
    "    }\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'text': [\"This is an excellent example of a positive sentence.\", \n",
    "                 \"This is a terrible example of a negative sentence.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute variables for each text\n",
    "variables_df = df['text'].apply(calculate_variables).apply(pd.Series)\n",
    "\n",
    "# Combine the original DataFrame with the variables DataFrame\n",
    "df_combined = pd.concat([df, variables_df], axis=1)\n",
    "\n",
    "print(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "150a6097-c563-42e0-b80b-4bf3a8c6c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "# These are the required parameters \n",
    "variables = [positive_score,\n",
    "            negative_score,\n",
    "            Polarity_Score,\n",
    "            subjectiivity_score,\n",
    "            avg_senetence_length,\n",
    "            Percentage_of_Complex_words,\n",
    "            Fog_Index,\n",
    "            avg_no_of_words_per_sentence,\n",
    "            complex_Word_Count,\n",
    "            word_count,\n",
    "            syllable_count,\n",
    "            personal_pronouns,\n",
    "            avg_word_length]\n",
    "\n",
    "# write the values to the dataframe\n",
    "for i, var in enumerate(variables):\n",
    "  output_df.iloc[:,i+2] = var\n",
    "\n",
    "#now save the dataframe to the disk\n",
    "output_df.to_csv('Output_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90502ceb-fafe-42bd-9bf6-401e883692c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
